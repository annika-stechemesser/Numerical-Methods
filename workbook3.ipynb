{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA934 Numerical Methods - Workbook 3\n",
    "\n",
    "If you haven't already done so, install the DualNumbers Julia package. It is a good idea to update all your packages first. The commands are\n",
    "\n",
    ">Pkg.update()\n",
    "\n",
    ">Pkg.add(\"DualNumbers\")\n",
    "\n",
    "but you only need to run them once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f_prime_ana (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pkg.update()\n",
    "#Pkg.add(\"DualNumbers\")\n",
    "using Plots\n",
    "using DualNumbers\n",
    "include(\"MODULES/MyUtils_wb3.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Numerical differentiation\n",
    "\n",
    "**1))** Derive a finite difference formula for the derivative of a function, $f$ at a point $x$ using the 3-point stencil $(x, x+h, x+2h)$ and state the order of the approximation error in terms of $h$.\n",
    "\n",
    "**2)** Write a formula for the derivative, $f^\\prime(x)$, of the function\n",
    "\n",
    "$$f(x) = \\sin(\\exp(x)) $$\n",
    "\n",
    "and evaluate it at $x=1$.\n",
    "\n",
    "**3)** Use your finite difference formula to approximate the value of $f^\\prime(1)$ for values of $h$ decreasing from $2^{-1}$ to $2^{-30}$ in powers of $2$. Plot the error as a function of $h$ and verify the theoretically predicted scaling of the error with $h$. What is the best relative error you can achieve?\n",
    "\n",
    "**4)** Read the examples at https://github.com/JuliaDiff/DualNumbers.jl. Define a dual number $x = 1+\\epsilon$ and use it to evaluate $f^\\prime(1)$. Verify that the answer is accurate to within machine precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1) \n",
    "\n",
    "We evaluate the function f(x) at the points of the stencil and taylor expand the function around these points:\n",
    "\n",
    "\\begin{align}\n",
    "f(x)&=f(x)\\\\\n",
    "f(x+h)&=f(x)+f'(x)h+f''(x)\\frac{h^2}{2}+\\mathcal{O}(h^3)\\\\\n",
    "f(x+2h)&=f(x)+2f'(x)h+2f''(x){h^2}+\\mathcal{O}(h^3)\n",
    "\\end{align}\n",
    "\n",
    "We define F as a linear combination of the stencil: \n",
    "\n",
    " $$F= A f(x) + B f(x+h) + C f(x+2h)$$\n",
    "\n",
    "\n",
    "We want to use $F$ to approximate $f'(x)$, so we have to solve the following system of linear equations: \n",
    "\n",
    "\\begin{align}\n",
    "A+B+C&=0\\\\\n",
    "B+2C&=1\\\\\n",
    "\\frac{B}{2}+2C&=0\n",
    "\\end{align}\n",
    "\n",
    "Solving this gives $ A=-3/2 , B=2 , C=-1/2$\n",
    "\n",
    "Finally, $F=f'(x)h+\\mathcal{O}(h^3)$. Rearranging this suggests that \n",
    "\n",
    "$$ f'(x) = \\frac{-3/2f(x)+2f(x+h)-1/2f(x+2h)}{h}+\\mathcal{O}(h^2)$$\n",
    "\n",
    "Therefore the order of the approximation error is $h^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) \n",
    "We want to evaluate the derivative of $f(x)=sin(exp(x))$ at $x=1$. \n",
    "\n",
    "$$ f'(x)=cos(exp(x))exp(x)$$\n",
    "\n",
    "For $x=1$ we therefore get \n",
    "\n",
    "$$ f'(1)=cos(e)e$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\n",
    "\n",
    "We use our formula for $f'(x)$ which we found in part 1 and evaluate it at $x=1$ for $h$ from $2^{-1}$ to $2^{-30}$ in powers of 2. We plot the error as a function of $h$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition f(Any) in module Main at /home/annika/Numerical_Methods/MODULES/MyUtils_wb3.jl:2 overwritten at In[3]:2.\n",
      "WARNING: Method definition f_prime_num(Any) in module Main at /home/annika/Numerical_Methods/MODULES/MyUtils_wb3.jl:7 overwritten at In[3]:7.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "f_prime_num (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function f(x)\n",
    "   y=sin(exp(x))\n",
    "    return y\n",
    "end\n",
    "\n",
    "function f_prime_num(h)\n",
    "    x=1\n",
    "    y=(-(3/2)*f(1)+2*f(x+h)-(1/2)*f(x+2*h))/h\n",
    "    return y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "der=cos(exp(1))*exp(1)\n",
    "h=zeros(30)\n",
    "z=zeros(30)\n",
    "for i=1:30\n",
    "    h[i]=2.0^(-i)\n",
    "    z[i]=abs(f_prime_num(h[i])-der)\n",
    "end\n",
    "\n",
    "plot(h,z,linewidth=2, yscale=:log2, xscale=:log2,xlabel=\"h\", label=\"Error\",title=\"Error as a function of h [log/log plot]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we find that the error is decreasing with h, as expected, but as h gets too small (less than ~$2^{-17}$) the floating point arithmetics cause a significant rounding error. In the decreasing region, we see that the plot is approximately linear (on a log/log scale). Therefore we expect the error to go as a power law (error ~$h^k$) where $k$ is the gradient of the straight line. To find this gradient we only consider h from $2^{-1}$ to $2^{-17}$ and use the function \"lingreg\" to fit a line (linear regression). We find that gradient of the line is 2.13 , which roughly matches our expectations from part 1 (i.e. error $\\mathcal{O}(h^2)$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best relative error (minimum error) we can achieve is the minimum of the error function displayed above which equals $2^{-33.5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(log2(findmin(z)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_cut=h[1:17]\n",
    "z_cut=z[1:17]\n",
    "h_cut_log=log2.(h_cut)\n",
    "z_cut_log=log2.(z_cut)\n",
    "a,b=linreg(h_cut_log,z_cut_log)\n",
    "println(\"the gradient of the line is\")\n",
    "println(b)\n",
    "plot(h_cut_log,z_cut_log,linewidth=2,label=\"numerical approximation\",xlabel=\"log2(h)\",ylabel=\"log2(error)\",title=\"How does the error scale with h?\")\n",
    "plot!(h_cut_log,b*h_cut_log+a,linewidth=2,label=\"best fit line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Finding roots\n",
    "\n",
    "**1)** Referring to the function, $f(x)$, defined above, find the roots of the equation\n",
    "\n",
    "$$ f(x) = 0$$\n",
    "\n",
    "in the interval $0<x<2$.\n",
    "\n",
    "**2)** Implement the bracketing and bisection method to find one of the roots numerically. Measure the error at each iteration of the algorithm and demonstrate that the error decreases exponentially as a function of the number of iterations. To how many digits of precision can you approximate the root?\n",
    "\n",
    "**3)** Perform the same measurements for the Newton Raphson method and show that the error decreases faster than exponentially as a function of the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1)\n",
    "We want to find the roots of the function $$f(x)=sin(exp(x))$$ \n",
    "\n",
    "For f(x) to be 0, $exp(x)$ has to be equal to $k\\pi$ where $k \\in \\mathcal{Z}$. Solving $exp(x)=k\\pi$ we get $x=log(k\\pi)$. As we're only interested in solutions for $x \\in [0,2]$, we solve $log(y)=0$ and $log(y)=2$ which gives us $k\\pi \\in [1, exp(2)\\sim 7.4]$. Therefore our roots are $log(\\pi)$ and $log(2\\pi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) \n",
    "We want to find numerically find the root $log(\\pi)$ of the function f(x) using the Bracketing and Bisection method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=1:1:45\n",
    "a=1.0\n",
    "b=1.2\n",
    "roots=zeros(length(steps))\n",
    "error=zeros(length(steps))\n",
    "for i in steps\n",
    "    x=(a+b)/2\n",
    "    if f(a)*f(x)>0 \n",
    "        a=x\n",
    "        b=b\n",
    "    else\n",
    "        a=a\n",
    "        b=x\n",
    "    end\n",
    "    roots[i]=x\n",
    "    error[i]=abs(log(pi)-x)\n",
    "end\n",
    "\n",
    "error_log=log10.(error)\n",
    "a,b=linreg(steps,error_log)\n",
    "println(\"Best fit line gradient =\")\n",
    "println(b)\n",
    "plot(steps,error_log,linewidth=2,xlabel=\"iteration\",ylabel=\"log10(error)\",title=\"Error of the Bracketing and Bisection Method\",label=\"log10(error)\")\n",
    "plot!(steps,b*steps+a,linewidth=2,label=\"best fit line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $log(\\pi)$ is approximately $1.145$ we choose our inital interval to be $[a,b]=[1.0,1.2]$. We iterate the algorithm 45 times. The number of iterations is set to 45 as a bigger number of iterations (e.g. 50) turned out not be sensible due to the floating point arithmetics. We take the log10 of the calculated error and plot this against the iterations number. The plot is approximately linear (see bestfit line, gradient = -0.3) which implies that the error decreases exponantially with the iteration number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(roots[45])\n",
    "log(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best estimate of the root is 1.1447298858493982. The first 12 decimal places equal the ones of the analytical root $log(\\pi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\n",
    "\n",
    "We now want to find the roots of the function using the Newton Raphson method. To do that, we use the analytics\n",
    "derivative that we found in Q1 part 2: \n",
    "\n",
    "$$f'(x)=\\cos(\\exp(x))\\exp(x)$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f_prime_ana(x)\n",
    "    y=cos(exp(x))*exp(x)\n",
    "    return y\n",
    "end\n",
    "\n",
    "steps=1:1:10\n",
    "x=1.0\n",
    "roots=zeros(length(steps))\n",
    "error=zeros(length(steps))\n",
    "for i in steps\n",
    "    d=-f(x)/f_prime_ana(x)\n",
    "    x=x+d\n",
    "    roots[i]=x\n",
    "    error[i]=abs(log(pi)-x)\n",
    "end\n",
    "println(\"The error values for the first 10 steps are\")\n",
    "println(error)\n",
    " error_log=log10.(error[1:4])\n",
    "plot(steps[1:4],error_log,linewidth=2,ylabel=\"log10(error)\",xlabel=\"iteration\",title=\"Error of the Newton Raphson Method\",label=\"log10(error)\")\n",
    "scatter!(steps[1:4],error_log,markersize=6,label=\"data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the root at $log(\\pi)$ so we choose an inital value of x=1.0. The Newton Raphon algorithm finds the root extremely fast. Printing the error value for the first ten steps (see above), we see that the error is \"equal\" to zero after step 4.  We therefore only plot the first four datapoints. Plotting log10(error) again the iterations we see that the curve decays faster than a linear function implying a superexponential error decay. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Finding minima\n",
    "\n",
    "**1)** The function $f(x)$ above has a single minimum in the interval $0<x<2$. Find its location analytically.\n",
    "\n",
    "**2)** Implement the Golden section search to find the location of this minimum numerically. Plot the error as a function of the number of iterations. To how many digits of precision can you approximate the location of the minimum?\n",
    "\n",
    "**3)** To understand your empirical findings, use Taylor's Theorem to show that near a minimum, $x_*$, of f(x),\n",
    "\n",
    "$$f(x) \\approx f(x_*)\\left( 1+ \\frac{f^{\\prime\\prime}(x_*)}{2\\,f(x_*)}\\,(x-x_*)^2\\right). $$\n",
    "Show that in order for a computer to distinguish between $f(x)$ and $f(x_*)$ we must have\n",
    "\n",
    "$$ \\left| x-x_*\\right| > \\sqrt{\\epsilon_m}\\,\\sqrt{\\left|\\frac{2\\,f(x_*)}{f^{\\prime\\prime}(x_*)}\\right|}$$\n",
    "\n",
    "thus limiting the precision with which the location of a minimum can be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
